# -*- coding: utf-8 -*-
"""Sentiment_imdb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vGQ_cH_vodhwnWMWwgY95cyrELWep64h
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy as sp
from sklearn import feature_extraction, model_selection, naive_bayes, metrics
from sklearn.model_selection import train_test_split
# %matplotlib inline
from sklearn.feature_extraction.text import CountVectorizer

# Deep Learing Preprocessing - Keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.utils import to_categorical

# Deep Learning Model - Keras
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential

# Deep Learning Model - Keras - CNN
from tensorflow.keras.layers import Conv1D, Conv2D, Convolution1D, MaxPooling1D, SeparableConv1D, SpatialDropout1D, GlobalAvgPool1D, GlobalMaxPool1D, GlobalMaxPooling1D 
from tensorflow.keras.layers import MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D

# Deep Learning Model - Keras - RNN
from tensorflow.keras.layers import Embedding, LSTM, Bidirectional

# Deep Learning Model - Keras - General
from tensorflow.keras.layers import Input, Add, concatenate, Dense, Activation, BatchNormalization, Dropout, Flatten
from tensorflow.keras.layers import LeakyReLU, PReLU, Lambda, Multiply



# Deep Learning Parameters - Keras
from tensorflow.keras.optimizers import RMSprop, Adam

# Deep Learning Callbacs - Keras
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau

import nltk
nltk.download('wordnet')

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/My Drive/imdb_dataset

dataset = pd.read_csv('IMDB Dataset.csv', encoding='latin-1')

dataset.groupby('sentiment').describe().T

def preprocess(word):
  word=re.sub(r'([a-z])\1+', r'\1',word)
  return word

def remove_break (word):
  if word == '<br />' :
    word=''

from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer()

import re
for index, line in dataset.iterrows():
  i=line[0].split()
  for word in range(len(i)):
    for ch in range(len(i[word])):
      checker=remove_break(i[word][ch])
      if(checker is not None):
        i[word]+=checker
    i[word]=preprocess(i[word])
    i[word]=lemmatizer.lemmatize(i[word])
  line[0]=' '.join(i)

X = dataset['review']
Y = dataset['sentiment']
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

Y = label_encoder.fit_transform(Y)

Y = Y.reshape(-1, 1)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=44)

max_words = len(set(" ".join(X_train).split()))
max_len = 250

tokenizer = Tokenizer(num_words=max_words)

tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_train_seq = sequence.pad_sequences(X_train_seq, maxlen=max_len)

cd ..

cd GloVe

embeddings_index = dict()
f = open(r"glove.6B.100d.txt",encoding="utf8")
for line in f:
    values = line.split()
    #print(values)
    word = values[0]
    #print(word)
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
print('Loaded %s word vectors.' % len(embeddings_index))

vocab_size = len(tokenizer.word_index) + 1
print(vocab_size)
embedding_matrix = np.zeros((vocab_size, 100))
for word, i in tokenizer.word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

print(embedding_matrix.shape)

model = Sequential()
model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=True))
model.add(Conv1D(256, 2, activation='relu'))
model.add(MaxPooling1D(2))
model.add(Dropout(0.3))
model.add(Conv1D(256, 2, activation='relu'))
model.add(MaxPooling1D(2))
model.add(Dropout(0.3))
model.add(Conv1D(128, 2, activation='relu'))
model.add(MaxPooling1D(2))
model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
# summarize the model
print(model.summary())

model.fit(X_train_seq, Y_train,
                    batch_size=512,
                    epochs=10,
                    verbose=1)

test_X_seq = tokenizer.texts_to_sequences(X_test)
test_X_seq = sequence.pad_sequences(test_X_seq, maxlen=max_len)
accuracy1 = model.evaluate(test_X_seq, Y_test)